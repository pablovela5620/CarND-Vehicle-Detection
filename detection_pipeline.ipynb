{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Flatten, Dense, Activation, Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Keras YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_image_dim_ordering('th')\n",
    "\n",
    "def make_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(3, 448, 448), border_mode='same', subsample=(1, 1)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))\n",
    "    model.add(Convolution2D(512, 3, 3, border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid'))\n",
    "    model.add(Convolution2D(1024, 3, 3, border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Convolution2D(1024, 3, 3, border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Convolution2D(1024, 3, 3, border_mode='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(4096))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dense(1470))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Prediction Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Box:\n",
    "    def __init__(self):\n",
    "        #center coordinates (x,y) of the bounding box relative to the grid cell\n",
    "        self.x, self.y = float(), float()\n",
    "        #width and height relative to the entire image\n",
    "        self.w, self.h = float(), float()\n",
    "        #confidence, IOU between the predicted and ground truth bounding box\n",
    "        self.c = float()\n",
    "        #class probability per grid cell, only one is made per cell regardless of number of boxes predicted\n",
    "        self.prob = float()\n",
    "\n",
    "def overlap(x1, w1, x2, w2):\n",
    "    #Subtract midpoint from width/height and maximize to find left bounding box\n",
    "    l1 = x1 - w1 / 2.\n",
    "    l2 = x2 - w2 / 2.\n",
    "    left = max(l1, l2)\n",
    "    #Same but minimize to find right bounding box\n",
    "    r1 = x1 + w1 / 2.\n",
    "    r2 = x2 + w2 / 2.\n",
    "    right = min(r1, r2)\n",
    "    #Subtract Right from Left to find overlap between bounding boxes\n",
    "    overlap = right - left\n",
    "    return overlap\n",
    "\n",
    "\n",
    "def box_intersection(box_a, box_b):\n",
    "    #find width and height of overlapping area between bbox a and b\n",
    "    width = overlap(box_a.x, box_a.w, box_b.x, box_b.w)\n",
    "    height = overlap(box_a.y, box_a.h, box_b.y, box_b.h)\n",
    "    #if overlap is negative, there is no overlap\n",
    "    if width < 0 or height < 0:\n",
    "        return 0\n",
    "\n",
    "    overlap_area = width * height\n",
    "    return overlap_area\n",
    "\n",
    "\n",
    "def box_union(box_a, box_b):\n",
    "    #Finds the area of union between box a and b\n",
    "    area_a = box_a.w * box_a.h\n",
    "    area_b = box_b.w * box_b.h\n",
    "    intersection = box_intersection(box_a, box_b)\n",
    "    union = area_a + area_b - intersection\n",
    "    return union\n",
    "\n",
    "\n",
    "def box_iou(box_a, box_b):\n",
    "    #Finds intersection over union for box a and b\n",
    "    return box_intersection(box_a, box_b) / box_union(box_a, box_b)\n",
    "\n",
    "\n",
    "\n",
    "def prediction_to_boundingboxes(prediction_vector, threshold=0.2, sqrt=1.8, C=20, B=2, S=7):\n",
    "    # Index of car class in the VOC dataset\n",
    "    car_class_number = 6\n",
    "\n",
    "    boxes = []\n",
    "\n",
    "    #The output of the YOLO network or the prediction vector is a 1470(S*S*(B*5+C)) vector that contains the information for predicting the bounding boxes\n",
    "    SS = S * S  # number of grid cells\n",
    "    prob_size = SS * C  # class probabilities 7*7*20=980\n",
    "    conf_size = SS * B  # confidences for each grid cell 7*7*2=98\n",
    "\n",
    "    #first part of vector where probability predictions are\n",
    "    probabilities = prediction_vector[0:prob_size]\n",
    "    #second part of vector where confidence are found\n",
    "    confidence_scores = prediction_vector[prob_size: (prob_size + conf_size)]\n",
    "    #last part of vector where bounding box predictions are\n",
    "    cords = prediction_vector[(prob_size + conf_size):]\n",
    "\n",
    "    # Reshape the arrays so that its easier to loop over them\n",
    "    probabilities = probabilities.reshape((SS, C))\n",
    "    confs = confidence_scores.reshape((SS, B))\n",
    "    cords = cords.reshape((SS, B, 4))\n",
    "\n",
    "    #Loop to find x,y,w,h,and c predictions from output vector\n",
    "    for grid in range(SS):\n",
    "        for b in range(B):\n",
    "            bx = Box()\n",
    "\n",
    "            bx.c = confs[grid, b]\n",
    "\n",
    "            #Converting position vectors from absolute value to relative\n",
    "            bx.x = (cords[grid, b, 0] + grid % S) / S\n",
    "            bx.y = (cords[grid, b, 1] + grid // S) / S\n",
    "\n",
    "            bx.w = cords[grid, b, 2] ** sqrt\n",
    "            bx.h = cords[grid, b, 3] ** sqrt\n",
    "\n",
    "            # multiply confidence scores with class probabilities to get class sepcific confidence scores\n",
    "            p = probabilities[grid, :] * bx.c\n",
    "\n",
    "            # Check if the confidence score for class 'car' is greater than the threshold\n",
    "            if p[car_class_number] >= threshold:\n",
    "                bx.prob = p[car_class_number]\n",
    "                boxes.append(bx)\n",
    "\n",
    "    # sort confidence score of each box in starting from largest to smallest\n",
    "    boxes.sort(key=lambda b: b.prob, reverse=True)\n",
    "\n",
    "    #remove overlapping boxes\n",
    "    for i in range(len(boxes)):\n",
    "        boxi = boxes[i]\n",
    "        #discards predicted box if probability is zero\n",
    "        if boxi.prob == 0:\n",
    "            continue\n",
    "\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            boxj = boxes[j]\n",
    "\n",
    "            # If boxes have 40% iou then the boxes with the highest probability is kept\n",
    "            if box_iou(boxi, boxj) >= 0.4:\n",
    "                boxes[j].prob = 0\n",
    "\n",
    "    boxes = [b for b in boxes if b.prob > 0]\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def draw_boxes(boxes, img, crop_dim=((500,1280),(300,650))):\n",
    "    drawn_img = img.copy()\n",
    "    #crop image so we dont draw boxes where there would be no cars, such as in the sky or in the lane past the barrier\n",
    "    [xmin, xmax] = crop_dim[0]\n",
    "    [ymin, ymax] = crop_dim[1]\n",
    "\n",
    "    height, width, _ = drawn_img.shape\n",
    "    for box in boxes:\n",
    "        w = xmax - xmin\n",
    "        h = ymax - ymin\n",
    "\n",
    "        left = int((box.x - box.w / 2.) * w) + xmin\n",
    "        right = int((box.x + box.w / 2.) * w) + xmin\n",
    "        top = int((box.y - box.h / 2.) * h) + ymin\n",
    "        bot = int((box.y + box.h / 2.) * h) + ymin\n",
    "\n",
    "        if left < 0:\n",
    "            left = 0\n",
    "        if right > width - 1:\n",
    "            right = width - 1\n",
    "        if top < 0:\n",
    "            top = 0\n",
    "        if bot > height - 1:\n",
    "            bot = height - 1\n",
    "\n",
    "        thick = 5\n",
    "        color = (0, 0, 255)  #blue\n",
    "\n",
    "        cv2.rectangle(drawn_img, (left, top), (right, bot), color, thick)\n",
    "\n",
    "    return drawn_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making model and showing model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import load_weights\n",
    "\n",
    "model = make_model()\n",
    "load_weights(model,'yolo-tiny.weights')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import preprocess\n",
    "\n",
    "def detection_pipeline(img):\n",
    "    pre_processed = preprocess(img)\n",
    "    #expand dimensions because input expects [1,3,448,448]\n",
    "    input = np.expand_dims(pre_processed, axis=0)\n",
    "    prediction = model.predict(input)\n",
    "    #Post process vector to extract bounding boxes\n",
    "    bboxes = prediction_to_boundingboxes(prediction[0],threshold=0.20)\n",
    "    #draw boxes\n",
    "    final_img = draw_boxes(bboxes,img)\n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gs\n",
    "\n",
    "def plot_all_imgs(input,output):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    gs1 = gs.GridSpec(nrows=1,ncols=2)\n",
    "    \n",
    "    ax1 = plt.subplot(gs1[0,0])\n",
    "    ax1.set_title('Original')\n",
    "    plt.imshow(input)\n",
    "    \n",
    "    ax2 = plt.subplot(gs1[0,1])\n",
    "    ax2.set_title('Bounding Boxes')\n",
    "    plt.imshow(output)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running pipeline on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = glob.glob('test_images/test*.jpg')\n",
    "for img in test_images:\n",
    "    input = mpimg.imread(img)\n",
    "    output = detection_pipeline(input)\n",
    "    plot_all_imgs(input,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pipeline on project video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video = 'video_output.mp4'\n",
    "output_clip = VideoFileClip('project_video.mp4')\n",
    "project_clip = output_clip.fl_image(detection_pipeline)  # NOTE: this function expects color images!!\n",
    "project_clip.write_videofile(output_video, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [You Only Look Once (YOLO) paper](https://arxiv.org/abs/1506.02640)\n",
    "- [Darknet](https://github.com/pjreddie/darknet)\n",
    "- [Darknet to Keras (YAD2K)](https://github.com/allanzelener/YAD2K)\n",
    "- [Xslittlegrass Implemintation](https://github.com/xslittlegrass/CarND-Vehicle-Detection)\n",
    "- [Subodh Malgonde Implemination](https://github.com/subodh-malgonde/vehicle-detection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}